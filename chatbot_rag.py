# -*- coding: utf-8 -*-
"""Chatbot_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v-8OYrSogK06RWMOWvUPAeB3i87BypPJ
"""

!pip install -qU openai langchain langchain-community tiktoken chromadb flask-ngrok


import os
from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA


os.environ["OPENAI_API_KEY"] = "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"

from google.colab import files
uploaded = files.upload()
filename = list(uploaded.keys())[0]


with open(filename, 'r', encoding='utf-8') as f:
    raw_text = f.read()

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_text(raw_text)


embedding = OpenAIEmbeddings()
vectordb = Chroma.from_texts(chunks, embedding=embedding)


llm = OpenAI()
qa = RetrievalQA.from_chain_type(llm=llm, retriever=vectordb.as_retriever())


while True:
    question = input(" You: ")
    if question.lower() in ["exit", "quit", "q"]:
        print(" Goodbye!")
        break
    try:
        answer = qa.run(question)
        print(" Bot:", answer)
    except Exception as e:
        print(" Error:", e)

